{
  "metadata": {
    "title": "The AI SRE on Call: How Traversal Solves the Toughest Incidents",
    "description": "When your system breaks, it can feel like a fire drill with dozens of engineers, dashboards open, and nothing but chaos. Enter Traversal—the AI Site Reliability Engineer (SRE) agent that works 24/7 to troubleshoot, fix, and even prevent incidents. By traversing petabytes of logs, metrics, and data across tools like Datadog, Grafana, Splunk, and more, Traversal delivers clarity when nothing else will. It handles the big, high-stakes incidents that cause real damage—and it does it before you maybe even knew there was a problem. It’s the trusted AI teammate that brings reliability back to your team. (Promise: No dashboard dumpster diving required.)",
    "author": "Webinar Maker Pro",
    "domain": "technical",
    "duration": 10,
    "created": "2025-08-19T04:35:40.132Z",
    "version": "1.0.0",
    "language": "en-US",
    "theme": {
      "primaryColor": "#2563eb",
      "secondaryColor": "#1e40af",
      "fontFamily": "JetBrains Mono, Consolas, monospace",
      "codeTheme": "github-dark"
    }
  },
  "slides": [
    {
      "id": "s1",
      "content": {
        "type": "title",
        "title": "From Chaos to Clarity: Why Incidents Need an AI SRE on Call",
        "subtitle": "An AI-Generated Presentation",
        "presenter": "Webinar Maker Pro",
        "date": "2025-08-19"
      },
      "speakerNotes": "- 0:00–0:10 Set the scene. Briefly describe the fire-drill chaos: too many alerts, too many dashboards, slow decisions\n- 0:10–0:25 Point to the first two bullets. Emphasize MELT + change + topology correlation across Datadog, Grafana/Prometheus, Splunk, Kubernetes, CI/CD\n- 0:25–0:40 Explain SLO-first prioritization and concise summaries; say the phrase “no dashboard dumpster diving” verbatim\n- 0:40–0:55 Safety angle. Call out guardrails, approvals, and auditability; reassure about human-in-the-loop for risky actions\n- 0:55–1:05 Quickly reference the PromQL snippet as familiar ground for the audience; note Traversal understands and uses these signals to decide when to act\n- 1:05–1:20 Walk through the Mermaid flowchart left-to-right: chaos → ingest → correlate → act → clarity\n- 1:20–1:30 Transition to the rest of the webinar: we’ll show real P0 scenarios and how Traversal shortens MTTD/MTTM/MTTR\n- Interaction: Ask a quick show-of-hands question: “Who’s been in a war room with 8+ dashboards open?” Pause half a beat for recognition\n- Technical reminder: Have Datadog/Grafana/Splunk logos visible on the slide; be ready to switch to demo later\n- Time check: Keep a brisk pace; do not deep-dive on the code yet—promise details in later sections",
      "narration": "When production breaks, it rarely looks like a clean puzzle... It’s a fire drill: hundreds of alerts, ten dashboards open, and a dozen people guessing... That chaos slows you down when every second matters... Traversal changes that... It’s an AI Site Reliability Engineer on call, twenty-four seven... It continuously ingests metrics, events, logs, and traces—plus deploys, config changes, and service topology—across tools you already use like Datadog, Grafana and Prometheus, Splunk, Kubernetes, and your CI and CD systems... Instead of paging on noise, Traversal prioritizes by SLO impact, builds a clear timeline, and forms a root-cause hypothesis with linked evidence... Then it recommends, or safely executes, the right mitigation: a rollback, a traffic shift, a circuit breaker, or a rate limit, all behind policy guardrails with human approval when needed... No dashboard dumpster diving required... Because it traverses petabytes of telemetry with context, it often catches problems before you even knew there was a user impact, and it learns from every incident to prevent the next one... The outcome is simple: lower time to detect, to mitigate, and to resolve; fewer pages; and fewer regressions... In the next sections, we’ll dive into how this works under the hood and walk through real, high-stakes incidents where Traversal turns war rooms into a fast, calm path from chaos to clarity.",
      "duration": 1.5,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s1.tsx",
      "audioPath": "/audio/slide-s1.mp3"
    },
    {
      "id": "s2",
      "content": {
        "type": "markdown",
        "title": "Foundations That Make AI Useful: SLOs, MELT, and Topology",
        "markdown": "- **Why these three matter:** SLOs define what “broken” means, MELT provides the evidence, topology explains impact and blast radius—together they turn AI from noise into action\n- **SLOs and error budgets:** Alert on burn rate tied to user journeys, not raw CPU; page only when budget is at risk; annotate alerts with owners and runbooks\n- **MELT unification:** Normalize metrics, events, logs, traces across Datadog, Grafana/Prometheus, Splunk via consistent tags and OpenTelemetry; include change events from CI/CD and flags\n- **Topology-first context:** Maintain a live service graph of microservices, DBs, queues, and external APIs; map ownership; correlate incidents to deploys and dependencies\n- **How Traversal uses it:** Prioritizes by SLO impact, correlates MELT by dependency path, links to changes, proposes guarded actions (rollbacks, traffic shifts), and explains decisions with evidence\n```yaml\n# Prometheus multi-window burn-rate alert for a checkout availability SLO\ngroups:\n- name: checkout-slo\n  rules:\n  - alert: CheckoutErrorBudgetBurn\n    expr: |\n      (sum(rate(http_requests_total{service=\"checkout\",status=~\"5..\"}[5m])) /\n       sum(rate(http_requests_total{service=\"checkout\"}[5m]))) > 0.02\n      and\n      (sum(rate(http_requests_total{service=\"checkout\",status=~\"5..\"}[1h])) /\n       sum(rate(http_requests_total{service=\"checkout\"}[1h]))) > 0.005\n    for: 5m\n    labels:\n      severity: page\n      slo: \"checkout-availability\"\n    annotations:\n      runbook: \"https://runbooks/checkout-slo\"\n      owners: \"@team-checkout\"\n```\n```mermaid\nflowchart LR\n  SLO[\"SLOs & error budgets\"] --> T[\"Traversal AI SRE\"]\n  MELT[\"MELT data (metrics, events, logs, traces)\"] --> T\n  TOPO[\"Topology graph & ownership\"] --> T\n  CHG[\"Change events (deploys, flags, infra)\"] --> T\n  TOOLS[\"Tools: Datadog, Grafana, Splunk\"] --> MELT\n  T --> DET[\"Detect & prioritize by SLO impact\"]\n  T --> MIT[\"Mitigate with guardrails (rollback, traffic shift)\"]\n  T --> EXP[\"Explain with linked evidence\"]\n```\n"
      },
      "speakerNotes": "- Spend 10s framing: these three foundations make AI actually useful in incidents\n- Spend 20s on SLOs: emphasize burn-rate alerting and error budgets guiding priority\n- Spend 20s on MELT: call out normalization via OpenTelemetry and pulling from Datadog, Grafana, Splunk; include change events\n- Spend 20s on topology: stress dependency graph, ownership, and change correlation\n- Spend 15s walking the Mermaid diagram: inputs into Traversal, outputs: detection, mitigation, explanation\n- Spend 15s on the YAML: show multi-window burn-rate alert and runbook/owner annotations\n- Spend 10s tie-back: how Traversal uses these to avoid dashboard diving and act safely\n- Stage directions: Switch to slide with Mermaid when explaining data flow; Point at SLO node then follow arrows; Highlight alert labels and annotations in YAML\n- Prompts: Ask “Who here pages on burn rate today?”; Pause briefly for quick show of hands\n- Reminder: Keep pace, avoid deep math; we’ll demo full RCA later\n",
      "narration": "If you remember one thing from today, make it this: SLOs, MELT, and topology are the foundations that make AI actually useful during incidents... SLOs define what broken means in terms your users feel... We alert on burn rate, not on raw CPU or a single 500 spike, so pages are about real impact and error budgets... MELT is the evidence... We unify metrics, events, logs, and traces from Datadog, Grafana and Prometheus, and Splunk, and we normalize them with consistent tags and OpenTelemetry so context flows across tools... Then topology connects the dots... A live service graph of microservices, databases, queues, and external APIs shows blast radius, ownership, and which changes touched what... Put these together and Traversal becomes the AI SRE you actually trust... It prioritizes by SLO impact, correlates anomalies along dependency paths, and ties symptoms to change events like a deploy or a feature flag... Instead of a hundred alerts, you get one clear incident with linked evidence and a safe recommendation: pause the canary, roll back the payment service, or shift traffic, with guardrails... The example alert on screen shows a multi window burn rate tied to our checkout SLO, annotated with owners and a runbook... That’s the pattern... Define the bar with SLOs, feed rich MELT, map the topology, and Traversal does the heavy lifting without the dashboard dumpster dive.",
      "duration": 1.5,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s2.tsx",
      "audioPath": "/audio/slide-s2.mp3"
    },
    {
      "id": "s3",
      "content": {
        "type": "markdown",
        "title": "Inside Traversal: Ingest, Correlate, Reason, and Act with Guardrails",
        "markdown": "- Ingest at scale: unify MELT + change + topology via connectors (Datadog, Grafana/Prometheus, Splunk, Kubernetes, CI/CD, feature flags), normalize tags/time/service identity\n- Correlate for signal: compress alert storms using SLO impact, anomaly detection, service topology, and change events; produce a single incident view\n- Reason with evidence: LLM + causal graph generate hypotheses, run counterfactual checks, and link to traces/logs/metrics that prove or disprove\n- Act safely: execute vetted runbooks (rollback, traffic shaping, rate limiting, flag toggles) with human-in-the-loop for high risk; every step auditable\n- Guardrails by policy: RBAC, approvals, blast-radius limits, canaries, timeouts, auto-rollback; trust built via precision/recall and win-rate metrics\n```yaml\n# OpenTelemetry Collector (ingest & normalize)\nreceivers:\n  otlp:\n    protocols: { grpc: {}, http: {} }\nexporters:\n  datadog:\n    api: { key: ${DATADOG_API_KEY} }\n  splunk_hec:\n    token: ${SPLUNK_HEC_TOKEN}\n    endpoint: https://splunk.example.com:8088\n  prometheusremotewrite:\n    endpoint: https://mimir.example.com/api/v1/push\n  kafka:\n    brokers: [\"kafka-1:9092\"]\nprocessors:\n  batch: {}\nservice:\n  pipelines:\n    traces:  { receivers: [otlp], processors: [batch], exporters: [datadog, kafka] }\n    metrics: { receivers: [otlp], processors: [batch], exporters: [datadog, prometheusremotewrite, kafka] }\n    logs:    { receivers: [otlp], processors: [batch], exporters: [splunk_hec, kafka] }\n```\n```rego\n# OPA guardrail (approve high-risk actions)\npackage traversal.guardrails\n\ndefault allow = false\n\nrequire_approval {\n  input.action.risk_score >= 3\n} or {\n  input.action.targets_percent > 10\n}\n\nallow {\n  not require_approval\n  input.caller.role == \"SRE\"\n  input.window == \"maintenance\"\n}\n\ndeny[msg] {\n  require_approval\n  msg := \"approval required: high risk or >10% traffic change\"\n}\n```\n```mermaid\nflowchart LR\nA[\"Ingest connectors: Datadog, Grafana, Splunk, K8s, CI/CD\"] --> B[\"Normalize: tags, time, service identity, topology\"]\nB --> C[\"Correlate: SLO impact, anomalies, change events\"]\nC --> D[\"Reason: LLM + causal graph; hypotheses + evidence\"]\nD --> E[\"Plan actions: runbooks, flags, traffic, rollbacks\"]\nE --> F{\"Guardrails: policy, RBAC, blast radius?\"}\nF -->|pass| G[\"Execute safely\"]\nF -->|needs approval| H[\"Human-in-the-loop\"]\nG --> I[\"Validate: SLOs stabilize, synthetic checks\"]\nI --> J[\"Learn: update detectors and runbooks\"]\n```\n"
      },
      "speakerNotes": "- 0:00 Open the slide and the Mermaid workflow. Set context: this is how Traversal replaces dashboard dumpster diving with an end-to-end loop.\n- 0:05 Spend 15 seconds on Ingest: call out connectors (Datadog, Grafana/Prom, Splunk, K8s, CI/CD), normalization of tags/time/service identity.\n- 0:20 Spend 20 seconds on Correlate: emphasize SLO impact, topology, and change events; mention alert storm compression to a single incident.\n- 0:40 Spend 20 seconds on Reason: LLM + causal graph, counterfactual checks, linked evidence; trust via transparent summaries.\n- 1:00 Spend 25 seconds on Act + Guardrails: runbooks, flags, traffic shaping, rollback; human-in-the-loop for high-risk; audit trail.\n- 1:25 Switch attention to the first code block (no need to zoom). Spend 20 seconds: highlight OTEL Collector pipelines to Datadog, Splunk, Grafana Mimir, Kafka.\n- 1:45 Move to the OPA snippet. Spend 15 seconds: show how approvals kick in for risk >= 3 or >10% traffic.\n- 2:00 Wrap: one line on outcomes (lower MTTD/MTTM/MTTR). Transition to next section or demo.\n- Prompts: Ask “Who already has SLOs and change events wired into alerts?” If time, quick show of hands.\n- Technical reminders: Keep the diagram visible; do not deep-dive the YAML; keep pace tight.\n",
      "narration": "Let’s look inside Traversal and how it moves from raw chaos to reliable action... First, ingest... Traversal continuously pulls in metrics, events, logs, and traces, plus change and topology data, from tools you already use—Datadog, Grafana and Prometheus, Splunk, Kubernetes, and your CI and feature flags... It normalizes tags, timestamps, and service identity so everything lines up.\nNext, correlate for signal... Instead of a thousand alerts, Traversal compresses the storm into a single incident by combining SLO impact, anomaly detection, service dependencies, and recent change events... You get one coherent picture and a ranked set of suspects—no dashboard dumpster diving.\nThen, reason with evidence... Traversal uses an LLM working alongside a causal graph to generate hypotheses, run quick counterfactual checks, and link to the exact traces, logs, and metrics that support or refute each idea... The output is a concise, transparent summary you can trust.\nFinally, act safely... Traversal executes known-good runbooks—like rollback, traffic shaping, rate limiting, or feature flag toggles—with human-in-the-loop for anything high risk... Every action is audited, validated with synthetic checks, and automatically rolled back if SLOs don’t recover.\nTwo quick glimpses under the hood... On ingest, an OpenTelemetry Collector pipeline fans telemetry to Datadog for metrics and traces, Splunk for logs, and Grafana Mimir via remote write, with Kafka for scale-out processing... On safety, an OPA policy requires approval when risk is high or when a change touches more than ten percent of traffic... Otherwise, low-risk actions can proceed within maintenance windows under RBAC.\nThis closed loop—ingest, correlate, reason, act with guardrails—cuts detection and mitigation time, handles the big incidents, and restores calm before you even knew there was a problem.",
      "duration": 2,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s3.tsx",
      "audioPath": "/audio/slide-s3.mp3"
    },
    {
      "id": "s4",
      "content": {
        "type": "markdown",
        "title": "Walkthrough: Deployment-Induced Outage Resolved in Minutes",
        "markdown": "- Incident: After deploy to checkout-service v531, p95 latency 3x and 5xx spike; SLO burn rate 14x; canary at 10% shows immediate degradation\n- Traversal response: Correlates ArgoCD deploy event with errors; surfaces diff (heap change + new flag); links Datadog errors, Grafana latency, Splunk stack traces\n- Safe plan: Freeze canary, rollback to v530, optionally disable feature flag checkout.optimisticWrite; human-in-the-loop approval with blast radius checks\n- Execution: Dry-run passes preflight; runbook executes rollback + flag toggle; continuous validation via synthetic checkout and SLO burn halt\n- Result: MTTR ~6 minutes; postmortem pack with timeline, trace exemplars, slow query hints; bug filed and runbook updated\n```yaml\n# Policy-as-code: guarded rollback plan executed by Traversal\nrunbook: rollback_checkout_v531\npreflight:\n  - check: slo_burn_rate < 2x after mitigation\n  - check: db_write_latency_p95 < 120ms\n  - check: capacity_in_region >= 2x current QPS\napprovals:\n  required_role: IncidentCommander\n  max_blast_radius: service=checkout, region=us-east-1\nsteps:\n  - name: freeze_canary\n    exec: argo rollouts pause rollout/checkout\n  - name: rollback_service\n    exec: kubectl rollout undo deployment/checkout --to-revision=530\n  - name: disable_flag\n    when: metric(\"errors_per_min\") > threshold\n    exec: |\n      curl -s -X PATCH https://flags.internal/api/flags/checkout.optimisticWrite \\\n        -H \"Authorization: Bearer $FLAG_TOKEN\" \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\"enabled\": false}'\n  - name: validate\n    exec: ./synthetics run checkout-flow --timeout 60s\n  - name: watch_slo\n    exec: ./dd query \"slo:checkout_availability burn_rate\" --expect \"< 1x\"\n``` \n```bash\n# Commands executed by Traversal (with audit trail)\n# 1) Correlate evidence\nopen https://app.datadoghq.com/apm/service/checkout/errors?from_ts=$T0&to_ts=$T1\nopen https://grafana/internal/d/latency/checkout?p95&from=$T0&to=$T1\nopen https://splunk/search?q=service=checkout error AND version=v531\n# 2) Dry-run and apply rollback\nargo rollouts pause rollout/checkout --namespace prod\nkubectl rollout undo deployment/checkout --to-revision=530 --namespace prod\n# 3) Optional flag mitigation\ncurl -s -X PATCH https://flags.internal/api/flags/checkout.optimisticWrite -d '{\"enabled\": false}' -H \"Authorization: Bearer $FLAG_TOKEN\" -H \"Content-Type: application/json\"\n# 4) Validate recovery\ntrv synthetics run checkout\ntrv slo status checkout\n``` \n```mermaid\nsequenceDiagram\n  participant CI as CI/CD\n  participant Obs as Datadog/Grafana/Splunk\n  participant Trav as Traversal\n  participant K8s as Kubernetes\n  participant IC as Incident Commander\n  CI->>Obs: Deploy v531 event + metrics/logs\n  Obs-->>Trav: 5xx spike, p95 up, traces\n  Trav->>Trav: Change correlation + RCA hypothesis\n  Trav-->>IC: Proposed plan (freeze, rollback, flag) & dry-run results\n  IC-->>Trav: Approve with policy guardrails\n  Trav->>K8s: Pause canary, rollout undo to v530\n  Trav->>Obs: Validate via synthetics + SLO burn stop\n  Trav-->>IC: Resolved summary + links + postmortem pack\n```"
      },
      "speakerNotes": "- 0:00 Show demo screen with incident card. Mention this is a real deployment-induced outage scenario.\n- 0:10 Click into the Traversal incident. Pause to highlight the SLO burn and correlated ArgoCD deploy event. Say \"no dashboard dumpster diving—evidence is linked.\"\n- 0:25 Open the Evidence panel. Briefly show Datadog errors, Grafana p95, and a Splunk stack trace. Keep it to 10 seconds.\n- 0:35 Switch to the Runbook tab. Scroll through preflight checks and guardrails. Emphasize human-in-the-loop approval.\n- 0:50 Click Dry Run. Narrate the checks (capacity, DB latency, synthetic pass). Confirm \"green\" checks.\n- 1:05 Click Approve & Execute. Verbally call out the actions: pause canary, rollback to v530, optional flag disable.\n- 1:20 Show live validation: synthetic checkout passes; SLO burn rate drops below threshold. Point to the before/after trend.\n- 1:35 Open the Postmortem pack: timeline, diffs, trace exemplars, filed bug link. Mention audit log.\n- 1:50 Summarize MTTR and that this ran with RBAC, approvals, and blast radius limits. Transition back to main session.\n- Technical reminders: Have Datadog/Grafana/Splunk tabs preloaded; ensure feature flag token in env; scale font on terminal; keep timestamps aligned.\n- Interaction prompt: Ask \"Would you trust this rollback with these guardrails?\" and note quick hands.\n- Time cues: Keep each step tight—no more than 10–15 seconds per click.\n- Contingency: If synthetic check is slow, narrate the expected result and show pre-recorded before/after screenshot.",
      "narration": "Let’s walk through a real deployment-induced outage resolved in minutes by Traversal... We’ve just rolled out version five thirty-one of the checkout service... Within seconds, Traversal detects a fourteen‑times SLO burn rate, p95 latency tripling, and a five‑hundred series spike isolated to the new version... Instead of jumping across dashboards, it correlates the ArgoCD deploy event with the anomaly, highlights the diff, and links me straight to the evidence in Datadog, Grafana, and Splunk, plus representative traces.\nHere’s the proposed, safe plan: freeze the canary, roll back to five thirty, and, if errors persist, disable the optimistic write feature flag... All of this is wrapped in policy guardrails—RBAC, an approval from the incident commander, blast radius limits to the checkout service in us‑east‑one, and preflight checks for capacity, database latency, and a quick synthetic checkout.\nI’ll run a dry‑run first... Checks are green, so I approve... Traversal pauses the canary, executes a kubectl rollout undo to revision five thirty, and toggles the flag only if error rates remain elevated... While it acts, it keeps watching synthetics and SLO burn... Within a minute, the synthetic flow turns green and the burn rate falls back under one‑times... That’s our confirmation to hold steady.\nAs a final step, Traversal assembles the postmortem pack: a precise timeline of the change, the heap configuration diff, links to the spike in logs and traces, and a filed bug for the checkout team... Total mean time to resolve: about six minutes, with full auditability and no dashboard dumpster diving... This is the AI SRE teammate taking the first, best action—safely, transparently, and fast.",
      "duration": 2,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s4.tsx",
      "audioPath": "/audio/slide-s4.mp3"
    },
    {
      "id": "s5",
      "content": {
        "type": "markdown",
        "title": "Rapid Scenarios: Regional Failover and Database Lock Storm",
        "markdown": "- Detects SLO burn, correlates provider status and topology to scope impact- Proposes safe, reversible actions; you approve or auto-run with guardrails- Validates via synthetics and SLOs; auto-rollback on regression- Full audit trail with links to Datadog, Grafana, and Splunk evidence\n\n```bash\n# Regional failover (safe plan)\ntraversal plan traffic-shift \\\n  --from us-east-1 --to us-west-2 \\\n  --canary 10 --ramp 10,25,50,100 \\\n  --synthetics checkout,login --slo-error-threshold 1.0\n# Execute after preview and approval\ntraversal execute --plan last --approve\n```\n\n```sql\n-- DB lock storm triage (Postgres example)\nSELECT a.pid, a.state, a.query, l.locktype, l.mode\nFROM pg_stat_activity a JOIN pg_locks l USING (pid)\nWHERE NOT l.granted ORDER BY a.query_start;\n-- Mitigation: terminate long blockers and add missing index\nSELECT pg_terminate_backend(pid) FROM pg_stat_activity\nWHERE state='active' AND now()-query_start > interval '30 seconds';\nCREATE INDEX CONCURRENTLY IF NOT EXISTS idx_orders_created_at ON orders(created_at);\n```\n\n```mermaid\nflowchart LR\nStart[\"Detect SLO burn\"] --> Branch{\"Which scenario?\"}\nBranch -->|\"Regional outage\"| R[\"Plan traffic shift\"]\nR --> P[\"Preflight synthetics\"]\nP --> E[\"Execute weighted DNS/service-mesh shift\"]\nBranch -->|\"DB lock storm\"| D[\"Rate-limit hot path; kill long blockers\"]\nD --> Q[\"Lock/slow query analysis; propose index or revert schema\"]\nE --> V[\"Validate SLO & error rate\"]\nQ --> V\nV -->|\"Stable\"| C[\"Auto-close; postmortem pack\"]\nV -->|\"Regressing\"| RB[\"Rollback & escalate\"]\n```\n"
      },
      "speakerNotes": "- Spend 5s framing: “Two rapid demos: regional failover and DB lock storm.”- Show demo (Regional) 25s: Open Traversal console with an active us-east incident. Highlight SLO burn and provider status correlation. Click “Preview plan” to show canary shift and synthetics. Switch to terminal and run the traffic-shift commands. Point out guardrails and rollback setting. Confirm execution and show synthetics green.- Show demo (DB) 25s: Switch to SQL tab/terminal. Run the lock query; highlight blockers. In Traversal, open proposed actions: rate-limit checkout API, terminate >30s blockers, create concurrent index. Approve rate-limit and index. Show metrics trend recovering.- Spend 5s wrap: Emphasize no dashboard dumpster diving, full audit trail linking Datadog/Grafana/Splunk, and auto-generated postmortem.- Technical reminders: Keep terminal font large; ensure creds preloaded; have a precomputed plan ready as fallback; keep an eye on clock; narrate guardrails explicitly.",
      "narration": "Let’s run two rapid scenarios to show Traversal in action—no dashboard dumpster diving... First, a regional disruption in us‑east... Traversal detects an SLO burn, correlates network errors in traces with the cloud provider status, and maps blast radius via the service graph... It generates a safe plan: canary shift ten percent of traffic to us‑west, run checkout and login synthetics, then ramp to one hundred percent if error rate stays below threshold... I preview the plan, approve, and it executes with guardrails and instant rollback if metrics regress... Now a database lock storm... Traversal flags high write latency and deadlocks right after a schema change... It surfaces the blocking and blocked queries, then proposes layered mitigation: throttle the hot endpoint, terminate long‑running blockers over thirty seconds, and create the missing index concurrently... I approve the rate limit and index... Traversal monitors queue drain and SLO recovery... Both incidents are fully audited with links to Datadog, Grafana, and Splunk, and a concise postmortem pack is generated automatically.",
      "duration": 1,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s5.tsx",
      "audioPath": "/audio/slide-s5.mp3"
    },
    {
      "id": "s6",
      "content": {
        "type": "markdown",
        "title": "Quick Pulse Check: Where Are Your Biggest Gaps?",
        "markdown": "- Quick chat poll: drop a number 1–5 for your biggest gap (we’ll tailor the demo to the top two)\n- 1: Alert noise or missing SLO burn-rate alerts\n- 2: Siloed telemetry across Datadog/Grafana/Splunk (dashboard dumpster diving)\n- 3: Slow triage/unknown root cause across microservice topology\n- 4: Missing safe automation/runbooks with guardrails\n- 5: Weak change correlation and rollback confidence\n```mermaid\nflowchart LR\nUser[\"You\"] --> Poll[\"Vote 1–5 in chat\"]\nPoll --> A1[\"1: Alert noise/SLOs\"] --> T1[\"Traversal: SLO burn-rate + alert correlation\"]\nPoll --> A2[\"2: Siloed telemetry\"] --> T2[\"Traversal: unified MELT + topology\"]\nPoll --> A3[\"3: Slow triage/RCA\"] --> T3[\"Traversal: topology-aware root cause analysis\"]\nPoll --> A4[\"4: Missing safe automation\"] --> T4[\"Traversal: guardrailed runbooks & approvals\"]\nPoll --> A5[\"5: Change correlation/rollback\"] --> T5[\"Traversal: change intelligence + auto-rollback\"]\n```\n```yaml\n# Example: SLO burn-rate alert (Prometheus)\nalert: CheckoutSLOBurn\nexpr: (sum(rate(http_requests_errors_total{job=\"checkout\"}[5m]))\n      / sum(rate(http_requests_total{job=\"checkout\"}[5m]))) > 0.02\nfor: 10m\nlabels:\n  severity: page\nannotations:\n  runbook: https://runbooks/checkout/slo-burn\n  summary: \"Checkout error budget burning >2% over 5m\"\n```"
      },
      "speakerNotes": "- Spend ~5 seconds framing: this is a quick pulse check to steer the demo\n- Ask audience: “Type 1–5 in chat now; I’ll pick the top two to focus our walkthrough.”\n- Read the room for ~10 seconds; call out the leading numbers\n- Tie results to Traversal capabilities: SLO correlation, unified MELT across Datadog/Grafana/Splunk, topology-aware RCA, guardrailed automation, change intelligence\n- If 2 or 5 wins, queue the deploy-induced outage demo; if 3 or 4 wins, queue DB saturation + runbook automation; if 1 wins, show SLO burn-rate detection\n- Technical reminder: have the relevant demo tab/terminal ready to switch quickly",
      "narration": "Quick pulse check... In chat, type a number one through five for your biggest gap... One is alert noise or missing SLO burn-rate alerts... Two is siloed telemetry across Datadog, Grafana, and Splunk... Three is slow triage or unclear root cause... Four is missing safe automation with guardrails... Five is weak change correlation and rollback confidence... We’ll prioritize the demo based on the top two... Traversal maps directly to each: SLO burn-rate correlation, unified MELT and topology, root cause on the service graph, guardrailed runbooks, and change intelligence... Drop your number now and I’ll adjust our walkthrough.",
      "duration": 0.5,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s6.tsx",
      "audioPath": "/audio/slide-s6.mp3"
    },
    {
      "id": "s7",
      "content": {
        "type": "markdown",
        "title": "Action Plan: Ship an AI SRE Safely—Checklist and KPIs",
        "markdown": "- Pilot one critical service first: define SLOs, propagate trace context, align tags; connect Datadog, Grafana/Prometheus, Splunk, Kubernetes, CI/CD change events\n- Safeguard actions: RBAC, approvals, blast radius limits; runbooks as code with dry-run, canary, rollback; immutable audit trail\n- Run a game day: trigger a safe failure; let Traversal detect→diagnose→propose; approve mitigations; verify SLO burn stops\n- Measure and iterate: track MTTD/MTTM/MTTR, alert precision/recall, pages per week, % auto-mitigated, toil hours saved; feed postmortems into detectors/runbooks\n```rego\n# Policy-as-code: require approval for high-risk production actions\npackage traversal.guardrails\n\ndefault allow = false\n\nallow {\n  input.env == \"prod\"\n  input.action.risk == \"low\"\n  input.actor.role == \"sre-bot\"\n}\n\n# Medium/high risk in prod requires human approval and small blast radius\nallow {\n  input.env == \"prod\"\n  input.action.risk != \"low\"\n  input.approvals >= 1\n  input.action.blast_radius <= 5  # e.g., percent traffic or node count\n  time.now_ns() >= input.change_window.start\n  time.now_ns() <= input.change_window.end\n}\n```\n```mermaid\nflowchart LR\n  A[\"Instrument & SLOs\"] --> B[\"Wire Datadog/Grafana/Splunk + change events\"]\n  B --> C[\"Import runbooks; enable guardrails\"]\n  C --> D[\"Game day: safe failure + human-in-the-loop\"]\n  D --> E[\"Measure KPIs & iterate\"]\n  E --> F[\"Expand blast radius gradually\"]\n```"
      },
      "speakerNotes": "- Spend 10 seconds framing: this is the concrete plan to ship Traversal safely and prove value fast\n- 20 seconds on first bullet: emphasize OpenTelemetry, consistent service tags, and wiring Datadog/Grafana/Splunk, Kubernetes, and CI/CD change events\n- 20 seconds on guardrails: point to policy-as-code snippet; call out approvals, blast radius, dry-run/canary/rollback, and audit logs\n- 20 seconds on game day: encourage a controlled failure; remind to use human-in-the-loop approvals; verify SLO burn rate drops after action\n- 20 seconds on KPIs: name MTTD/MTTM/MTTR, alert precision/recall, pages/week, % auto-mitigated, toil hours saved; mention feeding postmortems back\n- 10 seconds to walk the Mermaid flow from left to right; reinforce expanding only after KPIs improve\n- Prompt: ask the audience to pick one service they could pilot next week; quick show of hands\n- Technical reminder: if questions arise on policy, note this Rego runs in OPA or similar; Traversal integrates with existing RBAC\n- Close: invite them to start a two-week pilot and schedule a game day next week",
      "narration": "Let’s land this with a clear, safe action plan and the KPIs that prove it works... First, pilot Traversal on one high-impact service... Define your SLOs, make sure trace context propagates end to end, and align your tags and service names... Then plug in your existing tools: Datadog or Grafana and Prometheus for metrics and traces, Splunk for logs, Kubernetes APIs, and your CI or CD system for change events... Next, put guardrails around any automation... Use role-based access, explicit approvals, and blast radius limits... Runbooks should be code, with dry runs, canaries, and guaranteed rollbacks, and every action should leave an immutable audit trail... To build trust, schedule a game day... Trigger a safe, scoped failure, let Traversal detect, triage, and propose a mitigation, and approve the action with a human in the loop... Verify that your SLO burn rate slows or stops after the change... Measure and iterate... Track MTTD, MTTM, and MTTR, alert precision and recall, pages per week, the percentage of incidents auto mitigated, and toil hours saved... Feed what you learn in postmortems back into detectors and runbooks... Follow this path, expand the blast radius only as the KPIs improve, and you’ll get the benefits of an AI SRE on call—without the dashboard dumpster diving and without sacrificing safety.",
      "duration": 1.5,
      "transition": "fade",
      "voice": {
        "emotion": "friendly",
        "pace": "slow"
      },
      "componentPath": "./components/slides/Slide_s7.tsx",
      "audioPath": "/audio/slide-s7.mp3"
    }
  ],
  "transitions": {
    "default": "fade",
    "duration": 500
  }
}
